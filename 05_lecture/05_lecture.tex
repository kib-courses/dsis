\input{./../header/kib.tex}
\input{./../header/user.tex}

\title{Лекция 5. Feature Extraction для текстовых данных и построение эффективных моделей}

% \date{\today}
\date{29 октября 2019}
\author{Павел Владимирович Слипенчук }
\institute{Москва, МГТУ им.Бауманка,\\ каф.ИУ-8, \href{https://t.me/kibinfo}{КИБ}}
% \titlegraphic{\includegraphics[width=2cm]{logo_ur.jpg}}
\titlegraphic{\small \href{https://github.com/kib-courses/dsis}{Data Science для решения задач информационной безопасности}}

\begin{document}
  \maketitle
    
\begin{frame}{План лекции}
    \begin{enumerate}
    	\item \nameref{section:fe}
		\item \nameref{section:text_tasks}
		\item \nameref{section:texts_fe}
	\end{enumerate}
\end{frame}

\section{Feature Extraction}\label{section:fe}

\begin{frame}{Процесс DS}
	Процесс DS:
	\begin{enumerate}
		\item Определение Д.Н.К. -- Дано, Найти, Критерий
		\item Получение row data
		\item \termdef{Feature Extraction}: Анализ row data, формирование гипотез, проверка гепотиз, автоматизация.
		\item Машинное обучение: создание прототипа модели на тестовых данных. Оценка качества системы
		\item Data Engeneering: создание DS системы в целом
		\item Машинное обучение: донастройка модели
		\item Внедрение
	\end{enumerate}
\end{frame}

\begin{frame}{Feature Extraction}
	\termdef{Feature Extraction} 
	(Извлечение признаков, выделение признаков) 
	-- это творческий процесс
	направленный на анализ данных (raw data),
	извлечения из этих данных признаков
	и отбор из первоначального набора признаков 
	репрезентативных признаков.
	
	\begin{block}{Замечание}
	Так как задачи ИБ -- это задачи с \term{активным противником},
	сложные ML модели делать <<экономически не целесообразно>>,
	поэтому как правило упор делают именно на Feature Extraction,
	а не на машинном обучении.
	\end{block}
\end{frame}

\begin{frame}
	Иногда говорят, что Feature Extraction -- это последовательность трёх действий:
	\begin{enumerate}
		\item Feature Generation 
		\item Feature Transformation 
		\item Feature Selection
	\end{enumerate}
\end{frame}

\begin{frame}{Raw Data}
	\termdef{Raw data} (сырые данные) -- это искомые данные <<как есть>>, до процесса
	Feature Generation
	
	Примеры Raw data:
	\begin{enumerate}
		\item Текст
		\item Изображения
		\item Неприрывные сигналы, волны (например звук)
		\item User and entity data (для UEBA)
		\item интернет трафик
		\item Показатели (геоданные, датчики на производстве) 
	\end{enumerate}
	
	Это разделение условно. Например трек мыши можно воспринимать и
	как пару сигналов от времени: $x(t)$, $y(t)$ и как частный случай
	UEBA данных.
\end{frame}

\begin{frame}{Решающее дерево и решающий пень}
	\termdef{Решающий пень} -- это 
	\term{решающее дерево} с одним правилом.
	
	\auditorium{Зачем нужны решающие пни?}
	
	\auditorium{Верно ли, что любое решающее дерево можно предствить 
	в виде ансамблей решающих пней?}
	
\end{frame}

\begin{frame}{Feature Extraction и нейронные сети}
	\textbf{Нейронная сеть} --
	это последовательно-паралельный ансамбль 
	\term{решающих пней}.
	
	% TODO картинка
	
	Проблема нейронных сетей -- их построение. Т.е.
	как имея некую нейронную сеть её обучить?
	Backpropagation (Метод обратного распространения ошибки) 
	-- один из действенных методов для сильносвязных raw data
\end{frame}

\begin{frame}{Нейронная сеть}
	Нейронная сеть -- это автоматизация Feature Extraction.
	
	По причине сильной связности coсоедних признаков
	(соседние пиксели в изображении; значения сигнала в окрестности определённой)
	мы можем создать структуру автоматического извлечения неких паттернов.
	
	При большом количестве данных и при грамотных паттернах, нейронная сеть
	действительно обучится и даст весьма эффективные результаты.
	
	\
\end{frame}

\section{Задачи ИБ в текстах}\label{section:text_tasks}

\begin{frame}
	
\end{frame}

\section{Feature Extraction в текстовых данных}\label{section:texts_fe}


\section{Вопросы для самопроверки}

\end{document}